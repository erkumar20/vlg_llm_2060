# Summary and History :

The objective of this notebook is to participate in 'LLM : Detect AI generated text' competition where using a dataset of prompts and texts generated by humans and AI, we have to predict which one is generated by an AI. 
First, we use different datasets and combine them to prepare a dataset which has classes balanced. then , we clean tha data by stemming the text, removing stopwords and punctuations. After this, we convert text into numbers by Count Vectorizer and then finally we train an ML model on the final vector and test it on validation dataset before predicting the results of test dataset. 

version 14 : BERT is used instead of the above method

version 20 : DistilBERT used which worked way faster and was more accurate but threw an exception on submission, just like BERT

version 21 : Dummy test dataset used to test if the model throws an error on a large dataset. Works well but again throws an error when submitted, just like the previous BERT and DistilBERT models.

version 23 : Added DistilBERT model from Kaggle rather than importing it from Tensorflow

version 24 : Replaced Distilbert by BERT

version 25 : Increased the amount of training data

version 26 : Increasing the amount of training data increased the submission score. So, further increasing the training data.

version 27 : Increasing amount of data again increased the score. Making some minor changes like tweaking the learning rate and adding a couple of layers to the model

version 30 : Increasing amount of data further decreased the score. So, changing hyper parameters in the model with best performance till now.

version 31 : Increasing amount of data to almost 18000 entries stopped after 3 epochs to save from crashing but it gave better results in training with a higher auc for train and validation data with validation data having higher auc but when submitted, the score on hidden dataset is really low

version 32 : Replaced older data with [daigt-proper-train-dataset ](https://www.kaggle.com/datasets/thedrcat/daigt-proper-train-dataset?select=train_drcat_01.csv) . Look for older versions of this notebook to see the older dataset used.

version 33 : DistilBERT used instead of BERT on the new dataset

version 34 : DistilBERT gave notebook timeout error so reducing epochs to reduce notebook runtime.

version 36 : Didn't remove punctuations before feeding to DistilBERT
The results on using different ML models is documented in last cell.

version 37 : Forked version 27 and ran the code by removing the data cleaning part. Feeding raw data to the BERT model without removing punctuations, stopwords and without stemming resulted in an improved score of 0.787. So, feeding raw input to DistilBERT in this version.

Observation : Feeding raw inputs without removing punctuations and stop words and without stemming the data resulted in better scores

# File Description :

* BERTmodel.ipynb : The file has kaggle notebook of the version where BERT was used ( version 30)
* DistilBERT.ipynb : The file has kaggle notebook  of the version where DistilBERT was used (version 35)
* XGBoost.ipynb : The file of kaggle notebook of the version where XGBoost was used after vectorizing the data using count vectorizer ( version 8). The XGboost was simply replaced by SVM and logistic regression in later versions ( results listed below)


Results :
* XGBoost default : 0.276
* XGBoost learning rate 0.01 : 0.357
* XGBoost learning rate 0.005 : 0.442
* XGBoost learning rate 0.005 n_estimators 150 : 0.363
* SVM default : 0.325
* Logistic regression : 0.283
* DistilBERT learning rate 0.0001 : 0.369
* BERT learning rate 0.0001 6600 rows : 0.643
* BERT learning rate 0.0001 10660 rows (duplicates removed) : 0.668
* BERT learning rate 0.0005 10660 rows : 0.758
* BERT learning rate 0.0005 10660 rows without cleaning data (refer summary on first cell) : 0.758
* BERT learning rate 0.0007 11650 rows : 0.643
* BERT learning rate 0.0005 19000 rows : 0.499 (version 31)
* BERT learning rate 0.0005 DAIGT dataset : 0.484
* DistilBERT learning rate 0.0005 DAIGT dataset : 0.591
* DistilBERT learning rate 0.0005 DAIGT dataset without removing punctuation : 0.628
